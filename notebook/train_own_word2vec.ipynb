{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from flashtext import KeywordProcessor\n",
    "import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_sign = r\"\\ə\\ᴵ\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&\\\\…\\/\\{\\}\\''\\[\\]\\_\\/\\@\\$\\%\\^\\&\\*\\(\\)\\+\\#\\:\\!\\-\\;\\!\\\"\\\\(\\),\\.?'+`~$=|•！？。＂＃＄％＆＇（）＊＋，－／：；<>＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟‧﹏\"\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤', 'ə', '√',\n",
    "    'ᴵ', '∞', 'θ', '÷', 'α', '•', 'à', '−', 'β', '∅', '³', 'π', '‘', '₹', '´', '£', '€',\n",
    "    '×','™', '√', '²', '—', '…', ':', ';', '•', '！', '?', '$', '＄', '％', '＆', '（', '）', '-', '*']\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', \n",
    "                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', \n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', \n",
    "                'qoura': 'quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', \n",
    "                'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'em': 'them',\n",
    "                'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', \n",
    "                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'etherium': 'ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', '2k19':'2019', 'qouta': 'quota', \n",
    "                'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', \n",
    "                'demonitisation': 'demonetization', 'demonitization': 'demonetization', \n",
    "                'demonetisation': 'demonetization', 'pokémon': 'pokemon', 'n*gga':'nigga', 'p*':'pussy', \n",
    "                'b***h':'bitch', 'a***h****':'asshole', 'a****le-ish':'asshole', 'b*ll-s***':'bullshit', 'd*g':'dog', \n",
    "                'st*up*id':'stupid','d***':'dick','di**':'dick',\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "                \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n",
    "                \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora',\n",
    "                'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n",
    "                'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
    "                '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                'demonitization': 'demonetization', 'demonetisation': 'demonetization','\\u200b': ' ', '\\ufeff': '', 'करना': '', 'है': '',\n",
    "                'sh*tty': 'shitty','s**t':'shit',\n",
    "                'nigg*r':'nigger','bulls**t':'bullshit','n*****':'nigger',\n",
    "                'p*ssy':'pussy','p***y':'pussy',\n",
    "                'f***':'fuck','f*^k':'fuck','f*cked':'fucked','f*ck':'fuck','f***ing':'fucking', 'F*CKING': 'fucking',\n",
    "                'sh*t':'shit', 'su*k':'suck', 'a**holes':'assholes','a**hole':'asshole',\n",
    "                'di*k':'dick', 'd*ck': 'dick', 'd**k':'dick', 'd***':'dick',\n",
    "                'bull**it':'bullshit', 'c**t':'cunt', 'cu*t':'cunt', 'c*nt':'cunt','troʊl':'trool',\n",
    "                'trumpian':'bombast','realdonaldtrump':'trump','drumpf':'trump','trumpist':'trump',\n",
    "                \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n",
    "                'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n",
    "                'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n",
    "                'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n",
    "                'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n",
    "                'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n",
    "                'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n",
    "                'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n",
    "                'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop','ᴀ':'A',\n",
    "                'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation',\n",
    "                'doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers',\n",
    "                'negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',\n",
    "                'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term',\n",
    "                'RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you',\n",
    "                'trumpists': 'trump', 'trumpkins': 'trump','trumpism': 'trump','trumpsters':'trump','thedonald':'trump',\n",
    "                'trumpty': 'trump', 'trumpettes': 'trump','trumpland': 'trump','trumpies':'trump','trumpo':'trump',\n",
    "                'drump': 'trump', 'dtrumpview': 'trump','drumph': 'trump','trumpanzee':'trump','trumpite':'trump',\n",
    "                'chumpsters': 'trump', 'trumptanic': 'trump', 'itʻs': 'it is', 'donʻt': 'do not','pussyhats':'pussy hats',\n",
    "                'trumpdon': 'trump', 'trumpisms': 'trump','trumperatti':'trump', 'legalizefreedom': 'legalize freedom',\n",
    "                'trumpish': 'trump', 'ur': 'you are','twitler':'twitter','trumplethinskin':'trump','trumpnuts':'trump','trumpanzees':'trump',\n",
    "                'justmaybe':'just maybe','trumpie':'trump','trumpistan':'trump','trumphobic':'trump','piano2':'piano','trumplandia':'trump',\n",
    "                'globalresearch':'global research','trumptydumpty':'trump','frank1':'frank','trumpski':'trump','trumptards':'trump',\n",
    "                'alwaysthere':'always there','clickbait':'click bait','antifas':'antifa','dtrump':'trump','trumpflakes':'trump flakes',\n",
    "                'trumputin':'trump putin','fakesarge':'fake sarge','civilbot':'civil bot','tumpkin':'trump','trumpians':'trump',\n",
    "                'drumpfs':'trump','dtrumpo':'trump','trumpistas':'trump','trumpity':'trump','trump nut':'trump','tumpkin':'trump',\n",
    "                'russiagate':'russia gate','trumpsucker':'trump sucker','trumpbart':'trump bart', 'trumplicrat':'trump','dtrump0':'trump',\n",
    "                'tfixstupid':'stupid','brexit':'<a>','Brexit':'<a>',\n",
    "               }\n",
    "               \n",
    "\n",
    "mispell_dict2 = {'americanophobia': '<q>', 'klastri':'<s>','thisisurl':'<url>','magaphants':'<x>','cheetolini':'<c>','daesh':'<b>',\n",
    "                'trumpelthinskin':'<n>'}\n",
    "emoji_re = re.compile(u'['\n",
    "                        u'\\U00010000-\\U0010ffff' \n",
    "                        u'\\U0001F600-\\U0001F64F'\n",
    "                        u'\\U0001F300-\\U0001F5FF'\n",
    "                        u'\\U0001F30D-\\U0001F567'\n",
    "                        u'\\U0001F680-\\U0001F6FF'\n",
    "                        u'\\u2122-\\u2B55]', re.UNICODE)\n",
    "\n",
    "kp = KeywordProcessor(case_sensitive=True)\n",
    "                \n",
    "mix_mispell_dict = {}\n",
    "for k, v in mispell_dict.items():\n",
    "    mix_mispell_dict[k] = v\n",
    "    mix_mispell_dict[k.lower()] = v.lower()\n",
    "    mix_mispell_dict[k.upper()] = v.upper()\n",
    "    mix_mispell_dict[k.capitalize()] = v.capitalize()\n",
    "\n",
    "for k, v in mix_mispell_dict.items():\n",
    "    kp.add_keyword(k, v)    \n",
    "    \n",
    "\n",
    "kp2 = KeywordProcessor(case_sensitive=True)\n",
    "for k, v in mispell_dict2.items():\n",
    "    kp2.add_keyword(k, v)\n",
    "    \n",
    "\n",
    "def statistics_upper_words(text):\n",
    "    upper_count = 0\n",
    "    for token in text.split():\n",
    "        if re.search(r'[A-Z]', token):\n",
    "            upper_count += 1\n",
    "    return upper_count\n",
    "\n",
    "def statistics_unique_words(text):\n",
    "    words_set = set()\n",
    "\n",
    "    for token in text.split():\n",
    "        words_set.add(token)\n",
    "\n",
    "    return len(words_set)\n",
    "\n",
    "def statistics_characters_nums(text):\n",
    "\n",
    "    chars_set = set()\n",
    "\n",
    "    for char in text:\n",
    "        chars_set.add(char)\n",
    "    \n",
    "    return len(chars_set)\n",
    "\n",
    "def statistics_swear_words(text):\n",
    "    swear_count = 0\n",
    "    for swear_word in swear_words:\n",
    "        if swear_word in text:\n",
    "            swear_count += 1\n",
    "    return swear_count\n",
    "\n",
    "def content_preprocessing(text):\n",
    "    \n",
    "    \n",
    "    # text = text.lower()\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' thisisurl ', text)\n",
    "    text = emoji_re.sub(' ', text)\n",
    "    text = kp.replace_keywords(text)\n",
    "    text = kp2.replace_keywords(text)\n",
    "    # text = re.sub(\"[%s]+\" %punc_sign , ' ' ,text)\n",
    "    # emoji_num = len(emoji_re.findall(text))\n",
    "    text = clean_text(text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    # upper_count = statistics_upper_words(text)\n",
    "    # characters_num = statistics_characters_nums(text)\n",
    "    # unique_words_num = statistics_unique_words(text)\n",
    "    # swear_words_num = statistics_swear_words(text)\n",
    "\n",
    "            \n",
    "    # return text, swear_words_num, len(text.split()), emoji_num, upper_count, unique_words_num, characters_num\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1804874, 45)\n",
      "Test shape :  (97320, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "test_df = pd.read_csv(\"jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "train_df['comment_text'] = train_df['comment_text'].fillna('_##_').values\n",
    "test_df['comment_text'] = test_df['comment_text'].fillna('_##_').values\n",
    "\n",
    "# shuffling the data\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train shape : \", train_df.shape)\n",
    "print(\"Test shape : \", test_df.shape)\n",
    "\n",
    "\n",
    "### preprocessing\n",
    "# x_train, x_train_swear_words, x_train_token_num, x_train_emoji_num, x_train_upper_count, x_train_unique_words_num, x_train_characters_num = zip(*train_df[\"comment_text\"].apply(lambda x: content_preprocessing(x)))\n",
    "x_train = train_df[\"comment_text\"].apply(lambda x: content_preprocessing(x))\n",
    "\n",
    "# x_train_swear_words = np.array(x_train_swear_words, dtype=np.long).reshape(-1, 1)\n",
    "# x_train_token_num = np.array(x_train_token_num, dtype=np.long).reshape(-1, 1)\n",
    "# x_train_emoji_num = np.array(x_train_emoji_num, dtype=np.long).reshape(-1, 1)\n",
    "# x_train_upper_count = np.array(x_train_upper_count, dtype=np.long).reshape(-1, 1)\n",
    "# x_train_unique_words_num = np.array(x_train_unique_words_num, dtype=np.long).reshape(-1, 1)\n",
    "# x_train_characters_num = np.array(x_train_characters_num, dtype=np.long).reshape(-1, 1)\n",
    "\n",
    "y_aux_train = np.array(train_df[['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']])\n",
    "\n",
    "# x_test, x_test_swear_words, x_test_token_num, x_test_emoji_num, x_test_upper_count, x_test_unique_words_num, x_test_characters_num  = zip(*test_df[\"comment_text\"].apply(lambda x: content_preprocessing(x)))\n",
    "x_test = test_df[\"comment_text\"].apply(lambda x: content_preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116659279"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439856"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.DataFrame(x_train).values.tolist() + pd.DataFrame(x_test).values.tolist()\n",
    "\n",
    "for i, s in enumerate(sentences):\n",
    "    sentences[i] = s[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 300\n",
    "\n",
    "w2v_5 = Word2Vec(sentences, size = EMB_DIM, window = 5, min_count = 5, negative = 15, iter = 5, workers = multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Monday', 0.8106446862220764),\n",
       " ('Friday', 0.7989753484725952),\n",
       " ('Sunday', 0.789634108543396),\n",
       " ('Tuesday', 0.7655911445617676),\n",
       " ('Thursday', 0.7571506500244141),\n",
       " ('Sat', 0.7209328413009644),\n",
       " ('Wednesday', 0.6740602254867554),\n",
       " ('Tues', 0.6642817258834839),\n",
       " ('monday', 0.6493574976921082),\n",
       " ('afternoon', 0.6327422261238098)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.similar_by_word('Saturday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('folks', 0.847642183303833),\n",
       " ('individuals', 0.695095419883728),\n",
       " ('Americans', 0.6914371252059937),\n",
       " ('Canadians', 0.655942976474762),\n",
       " ('ppl', 0.6436157822608948),\n",
       " ('men', 0.6315521001815796),\n",
       " ('citizens', 0.6169369220733643),\n",
       " ('persons', 0.6133850812911987),\n",
       " ('People', 0.6105722188949585),\n",
       " ('folk', 0.6077068448066711)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.similar_by_word('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.save(\"word2vec_toxic.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_5 = w2v_5.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Monday', 0.8308067321777344),\n",
       " ('Friday', 0.8088524341583252),\n",
       " ('Sunday', 0.7910346984863281),\n",
       " ('Tuesday', 0.7748814225196838),\n",
       " ('Thursday', 0.7702040672302246),\n",
       " ('Sat', 0.7296500205993652),\n",
       " ('monday', 0.6901320219039917),\n",
       " ('Wednesday', 0.6893444061279297),\n",
       " ('mornings', 0.6730568408966064),\n",
       " ('afternoon', 0.6682068109512329)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_5.similar_by_word('Saturday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('folks', 0.8420507907867432),\n",
       " ('ppl', 0.6911355257034302),\n",
       " ('individuals', 0.6901845932006836),\n",
       " ('Americans', 0.6849813461303711),\n",
       " ('Canadians', 0.6536723971366882),\n",
       " ('men', 0.635567843914032),\n",
       " ('persons', 0.614833414554596),\n",
       " ('folk', 0.6124849319458008),\n",
       " ('citizens', 0.6069498062133789),\n",
       " ('People', 0.6041996479034424)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_5.similar_by_word('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load w2v pretrained model\n",
    "mod = Word2Vec.load(\"word2vec_5_toxic.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1354041e+00,  2.0508060e+00, -1.5045446e+00, -1.8079064e+00,\n",
       "       -7.2623053e+00,  3.2742286e+00, -1.7727764e+00,  1.6742339e+00,\n",
       "       -2.1809361e+00, -1.4824820e+00, -1.6520038e-01, -8.7806219e-01,\n",
       "        2.1176753e+00, -1.9665916e+00,  4.0627003e+00,  1.2337866e+00,\n",
       "       -5.0124055e-01, -1.4412597e+00, -4.0134898e-01,  5.8231723e-01,\n",
       "       -5.8083099e-01,  1.4449546e+00, -3.1821087e-01, -5.9059674e-01,\n",
       "       -1.0887691e+00,  1.0027692e+00, -1.3858904e+00, -1.6214668e+00,\n",
       "       -3.6171106e-01, -2.1032393e+00, -2.6351700e+00, -8.1643164e-01,\n",
       "        7.2800565e-01, -2.3075898e-01, -2.5022948e+00,  3.3966476e-01,\n",
       "        3.2912934e+00, -9.7948372e-01, -4.3140543e-01, -6.7572999e-01,\n",
       "        1.4833511e+00, -1.3219533e+00, -1.0327908e+00,  1.0059941e+00,\n",
       "        1.8049970e+00,  1.4624935e-01, -6.9917518e-01, -1.3378119e+00,\n",
       "        3.5584950e+00,  8.9679855e-01,  1.7294911e+00,  7.5850630e-01,\n",
       "        4.8537707e+00, -6.1280439e-03, -2.1086326e+00, -1.1157238e+00,\n",
       "        1.1993729e+00,  1.1683440e+00,  6.7023844e-01, -5.1565218e-01,\n",
       "        9.0790164e-01, -2.1368307e-01,  3.7781242e-01,  9.1907227e-01,\n",
       "       -7.4012125e-01,  1.7702195e-01, -1.7901313e+00,  6.5361798e-01,\n",
       "       -2.1422932e+00,  4.8427030e-01,  2.1854134e+00, -3.1540449e+00,\n",
       "        1.7350719e+00, -1.7517588e+00,  3.1716815e-01, -7.4356727e-02,\n",
       "       -1.3160999e-01,  3.1129651e+00,  3.9941064e-01,  4.5189819e-01,\n",
       "       -1.6555591e+00,  1.2797548e+00,  1.6346190e+00,  2.2573891e-01,\n",
       "        2.4901538e-01,  2.0215571e+00, -1.4252280e+00,  1.3288091e+00,\n",
       "       -2.2965634e+00, -6.4470398e-01, -3.0427837e-01,  4.2291071e-02,\n",
       "        2.9524221e+00, -2.1314671e+00, -1.3581469e+00,  8.3791447e-01,\n",
       "       -9.6460927e-01, -1.1137478e+00,  4.3903354e-01,  3.1372313e+00,\n",
       "       -2.9373224e+00,  3.3369293e+00,  9.5864272e-01, -1.8772565e+00,\n",
       "        1.1045852e+00, -3.6398074e-01,  1.0616840e+00, -3.6125115e-01,\n",
       "        1.0086497e+00,  5.4445267e-01, -8.6686170e-01, -8.1384873e-01,\n",
       "        8.3114672e-01,  3.3834136e-01,  2.6816851e-01,  2.1634645e+00,\n",
       "       -2.5688438e+00,  4.0210497e-01, -3.8184446e-01,  9.3213570e-01,\n",
       "       -1.6627667e+00, -3.7926304e-01,  4.6232909e-01, -5.9868807e-01,\n",
       "       -5.2952063e-01,  6.0514230e-01, -4.6542907e-01, -1.2732420e+00,\n",
       "       -2.2568266e+00,  1.0079126e+00, -9.4806212e-01,  9.4860423e-01,\n",
       "       -1.5223675e+00, -1.4401314e-01,  6.6659778e-01,  1.9006401e+00,\n",
       "       -6.1560857e-01, -1.0771253e+00,  1.2381166e+00,  2.6170892e-01,\n",
       "        1.4667531e+00,  9.0808511e-01, -1.1838509e+00,  1.9470131e+00,\n",
       "       -2.7674360e+00, -2.1045327e+00,  5.6197309e-01,  1.9656586e-02,\n",
       "        2.0161057e+00,  2.4088871e-01,  1.1505609e+00,  9.3796819e-01,\n",
       "        3.2176292e-01, -1.0575713e+00, -4.1347728e+00,  2.5961521e+00,\n",
       "        3.8377121e-01,  6.7309517e-01,  2.3402867e-01,  7.5753051e-01,\n",
       "       -2.3645248e+00,  1.8670393e+00, -9.7358751e-01,  1.1815317e+00,\n",
       "       -5.6929765e+00, -2.0176481e-02, -5.8198833e-01,  1.0676700e+00,\n",
       "       -7.2962499e-01, -1.3891006e-01, -1.3348506e-01, -3.5533230e+00,\n",
       "        2.3469663e-01,  1.7578957e+00,  3.2613495e-01,  3.5255485e+00,\n",
       "        1.5393205e-01,  2.0810766e+00, -8.6697179e-01,  8.0206901e-01,\n",
       "        7.5540841e-01, -2.8414092e+00, -3.8932447e+00, -4.1812652e-01,\n",
       "       -1.1509647e+00,  6.7079115e-01,  2.5474937e+00,  2.1262417e+00,\n",
       "       -2.7039531e-01, -1.1638674e+00, -2.4146099e+00, -3.2823953e-01,\n",
       "       -2.1431928e+00,  1.1570295e+00, -7.7921081e-01,  4.3038926e-01,\n",
       "        1.6737907e+00, -1.2500011e+00,  1.4050344e+00, -5.9851432e-01,\n",
       "        8.9494419e-01, -1.0514621e+00, -2.7768072e-01, -5.1754236e-01,\n",
       "        1.4090321e+00,  8.8554192e-01, -1.4705952e+00,  1.0944551e+00,\n",
       "        1.9830005e+00,  1.1660621e+00,  1.1946810e+00, -1.1469659e+00,\n",
       "        1.9428512e-01, -3.7203808e+00,  1.2355330e+00,  2.3485962e-01,\n",
       "       -2.5911891e+00, -1.2237389e+00, -8.6546892e-01, -1.9505130e+00,\n",
       "       -3.2952590e+00, -1.2542723e+00,  2.1276052e-01,  4.8330888e-01,\n",
       "       -1.4449238e+00, -7.4344546e-01, -1.0436500e+00, -9.8261285e-01,\n",
       "        3.7759295e+00, -1.8427161e+00, -2.9555202e+00,  3.0095801e+00,\n",
       "        3.2846618e-01,  1.3463863e+00, -2.0107949e+00, -5.6865656e-01,\n",
       "        2.1490598e+00, -8.1725621e-01, -3.8550005e+00, -7.5653329e-02,\n",
       "       -2.1943340e-01, -1.9330009e+00,  2.9833311e-01,  2.2677290e+00,\n",
       "       -5.5258089e-01,  2.1592884e+00, -2.8466412e-01, -8.7143831e-02,\n",
       "       -2.6084902e+00,  1.4254051e+00, -1.7944545e+00,  1.3444469e+00,\n",
       "       -2.9758888e-01, -1.3704681e-01, -5.6364310e-01, -9.2423075e-01,\n",
       "       -6.9512832e-01, -1.8609996e+00, -2.1048248e+00, -7.0296383e-01,\n",
       "       -1.6435213e+00,  1.5538596e+00, -1.0592362e+00,  4.5164067e-01,\n",
       "        4.3169290e-01, -4.3850799e+00,  4.6243453e+00,  5.3231591e-01,\n",
       "        2.7615353e-01, -1.5906254e+00, -5.1249886e-01,  1.4391928e+00,\n",
       "       -2.1855177e-01,  5.8112693e-01, -8.6817485e-01, -2.9688036e-01,\n",
       "        1.0764650e+00,  5.6626624e-01,  1.0166506e+00, -1.9603853e+00,\n",
       "       -8.3918166e-01, -2.8839011e-02, -3.6319044e-01,  3.5725588e-01,\n",
       "       -4.9345854e-01,  1.3425263e+00,  8.1905484e-01,  2.5009997e+00,\n",
       "        6.0727715e+00, -3.0847257e-01,  7.9443818e-01, -2.2075689e-01,\n",
       "       -1.9205748e+00, -6.9520271e-01,  1.7832305e+00, -3.0465453e+00,\n",
       "        6.8856835e-02, -1.4240694e+00, -2.0944827e+00, -1.1445300e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.wv.word_vec('a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
